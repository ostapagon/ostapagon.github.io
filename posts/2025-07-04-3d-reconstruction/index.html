<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3D Reconstruction: From Hours to Minutes | ostapagon's Blog</title>
<meta name="keywords" content="TODO" />
<meta name="description" content="TODO">
<meta name="author" content="Ostap Hembara">
<link rel="canonical" href="http://127.0.0.1:4000/posts/2025-07-04-3d-reconstruction/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.51b2420ff5ea1215cdf584af7ba59d5fea94201c33f25109d6448c7271631316.css" integrity="sha256-UbJCD/XqEhXN9YSve6WdX&#43;qUIBwz8lEJ1kSMcnFjExY=" rel="preload stylesheet" as="style">
<link rel="stylesheet" href="/assets/css/custom.css">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js" integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY="
    onload="hljs.initHighlightingOnLoad();" onerror="loadHighlightJSFallback();"></script>
<link rel="icon" href="https://ostapagon.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ostapagon.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ostapagon.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ostapagon.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ostapagon.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://127.0.0.1:4000/posts/2025-07-04-3d-reconstruction/" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
.post-header h1 {
    font-size: 35px;
}
.post pre,
.post code {
    background-color: #fcfcfc;
    font-size: 13px; /* make code smaller for this post... */
}

/* Enhanced syntax highlighting CSS */
.post pre code {
    background-color: transparent !important;
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
    line-height: 1.4;
}

/* Unified syntax highlighting using dark theme palette for both light and dark modes */
.hljs-keyword { color: #ff7b72 !important; font-weight: bold !important; }
.hljs-string { color: #a5d6ff !important; }
.hljs-number { color: #79c0ff !important; }
.hljs-comment { color: #8b949e !important; font-style: italic !important; }
.hljs-function { color: #d2a8ff !important; }
.hljs-class { color: #d2a8ff !important; }
.hljs-name { color: #d2a8ff !important; }
.hljs-built_in { color: #79c0ff !important; }
.hljs-literal { color: #79c0ff !important; }
.hljs-params { color: #e6edf3 !important; }
.hljs-attr { color: #ff7b72 !important; }
.hljs-title { color: #d2a8ff !important; font-weight: bold !important; }
.image-row {
    display: flex;
    justify-content: center;
    align-items: flex-start;
    gap: 20px;
    margin: 20px 0;
    flex-wrap: nowrap;
}
.image-row img {
    display: block;
    margin: 0 auto;
}
.image-container {
    display: flex;
    flex-direction: column;
    align-items: center;
    text-align: center;
    flex: 0 0 auto;
}
.image-caption {
    margin-top: 10px;
    font-style: italic;
    color: #666;
    font-size: 16px;
}

.image-container img[src="diffuse_reflection.jpg"] {
    margin-left: 20px;
}

/* Footer links styling - make them display horizontally */
.footer-links {
    display: flex;
    justify-content: center;
    gap: 30px;
    flex-wrap: wrap;
    margin-bottom: 20px;
}

.footer-links a {
    text-decoration: none;
    color: inherit;
    transition: color 0.2s ease;
    font-weight: 500;
}

.footer-links a:hover {
    color: #007acc;
}


/* .logo {
    margin-left: -100px;
} */
</style>

</head>

<body class="" id="top">
    <noscript>
        <style>
            #theme-toggle,
            .top-link {
                display: none;
            }
    
        </style>
        <style>
            @media (prefers-color-scheme: dark) {
                :root {
                    --theme: rgb(29, 30, 32);
                    --entry: rgb(46, 46, 51);
                    --primary: rgb(218, 218, 219);
                    --secondary: rgb(155, 156, 157);
                    --tertiary: rgb(65, 66, 68);
                    --content: rgb(196, 196, 197);
                    --hljs-bg: rgb(46, 46, 51);
                    --code-bg: rgb(55, 56, 62);
                    --border: rgb(51, 51, 51);
                }
    
                .list {
                    background: var(--theme);
                }
    
                .list:not(.dark)::-webkit-scrollbar-track {
                    background: 0 0;
                }
    
                .list:not(.dark)::-webkit-scrollbar-thumb {
                    border-color: var(--theme);
                }
            }
    
            /* Social icons styling */
            .lang-switch {
                display: flex;
                align-items: center;
                gap: 8px;
            }
            
            .lang-switch li {
                display: flex;
                align-items: center;
            }
            
            .lang-switch a {
                display: flex;
                align-items: center;
                padding: 4px;
            }
            
            .lang-switch svg {
                width: 16px;
                height: 16px;
            }
    
        </style>
</noscript>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-NRCFMZDRR3"></script>
        <script>
        var doNotTrack = false;
        if ( false ) {
            var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
            var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-NRCFMZDRR3');
        }
        </script><meta property="og:title" content="ostapagon: Your Blog Title" />
<meta property="og:description" content="ostapagon: Your blog description" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://ostapagon.github.io/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ostapagon: Your Blog Title"/>
<meta name="twitter:description" content="ostapagon: Your blog description."/>

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "ostapagon: Your Blog Name",
    "url": "ostapagon: Your site URL",
    "description": "ostapagon: Your blog description",
    "thumbnailUrl": "ostapagon: Your favicon path",
    "sameAs": [
        "https://github.com/ostapagon",
        "https://twitter.com/ostapagone"
    ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
    MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
    },
    options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ostapagon.github.io/" accesskey="h" title="ostapagon">ostapagon</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="yin-yang-icon" class="yin-yang-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 100 100">
                        <circle cx="50" cy="50" r="48" fill="#fff" stroke="currentColor" stroke-width="4"/>
                        <path d="M50,2
                                 a48,48 0 1,1 0,96
                                 a24,24 0 1,0 0,-48
                                 a24,24 0 1,1 0,-48z" fill="#000"/>
                        <circle cx="50" cy="26" r="10" fill="#000"/>
                        <circle cx="50" cy="74" r="10" fill="#fff"/>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ostapagon.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://ostapagon.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
        3D Reconstruction: From Hours to Minutes

    <div class="post-meta">Date: Jul 04, 2025  |  Estimated Reading Time: min  |  Author: Ostap Hembara

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#rendering-3d-scene" aria-label="Rendering a 3D Scene">Rendering a 3D Scene</a></li>
                <li>
                    <a href="#reconstructing-3d-scene" aria-label="Reconstructing 3D Scene">Reconstructing 3D Scene</a>
                </li>
                    <a href="#3d-point-clouds-with-colmap" aria-label="3D Point Clouds with COLMAP">3D Point Clouds with COLMAP</a></li>
                <li>
                    <a href="#modern-alternatives-dust3r-and-mast3r" aria-label="Modern Alternatives: DUST3R and MASt3R">Modern Alternatives: DUST3R and MASt3R-SFM</a></li>                
                <li>
                    <a href="#nerfs-neural-radiance-fields" aria-label="NeRFs: Neural Radiance Fields">NeRFs: Neural Radiance Fields</a></li>
                <li>
                    <a href="#3d-gaussian-splatting" aria-label="3D Gaussian Splatting">3D Gaussian Splatting</a></li>
                <li>
                    <a href="#from-theory-to-practice" aria-label="From Theory to Practice">From Theory to Practice</a></li>
            </ul>
        </div>
    </details>
</div>

<div class="post-content">

<h2 id="rendering-3d-scene">Rendering a 3D Scene</h2>

<p>At its core, computer graphics is about creating two-dimensional images from three-dimensional virtual worlds. This process, known as rendering, is a fascinating blend of geometry, linear algebra, and physics simulation. Let's break down how we go from an abstract 3D object to a final, viewable image.</p>

<h3>The Building Blocks: 3D Objects as Meshes</h3>

<p>Every object you see in a game or an animated film starts as a collection of points in 3D space. These points are called <strong>vertices</strong>, each having a specific $(x, y, z)$ coordinate. To create a solid surface, we connect these vertices to form flat polygons, which are called <strong>faces</strong>. Most commonly, these faces are triangles or quadrilaterals (quads).</p>

<p>This collection of vertices and faces is known as a <strong>3D mesh</strong>. Think of it as a digital sculpture or a blueprint that defines the shape and structure of any object in the 3D world, whether it's a simple cube or a complex character.</p>

<div class="image-row">
    <div class="image-container">
        <img src="3d_object.png" width="600"/>
        <div class="image-caption">Elements of 3D objects: vertices, edges, faces, and surfaces.<br>Image from <a href="https://en.wikipedia.org/wiki/Polygon_mesh#/media/File:Mesh_overview.svg" target="_blank">Wikipedia</a>.</div>
    </div>
</div>

<h3>The Observer: The Virtual Camera</h3>

<p>To see our 3D mesh, we need a virtual camera. Just like a real-world camera, its purpose is to capture a specific view of the scene. In computer graphics, a camera's properties are defined by two crucial sets of information, often represented by matrices:</p>

<ul>
    <li><strong>World-to-Camera (W2C) Matrix:</strong> This is an extrinsic matrix that defines the camera's position and orientation in the 3D world. It answers the questions: Where is the camera located? And which direction is it pointing? It transforms the coordinates of all objects from the global "world space" into "camera space," where the camera is effectively at the origin (0,0,0) and looking down a specific axis (like the negative Z-axis).</li>
    
    <li><strong>Camera-to-View (C2V) Matrix:</strong> This is an intrinsic matrix, also known as the projection matrix. It defines the camera's internal properties, like the lens. It dictates how the 3D scene is flattened onto a 2D plane, creating the sense of perspective. Key attributes controlled by this matrix include the field of view (acting like a zoom lens), the aspect ratio (matching the final image dimensions), and the near and far clipping planes (defining the range of visible depths).</li>
</ul>

<h3>The Rendering Process: From 3D to 2D</h3>

<p>With our 3D mesh and camera in place, the rendering process can begin. It's fundamentally a two-step procedure that uses the matrices we just discussed to transform our 3D data into a 2D image.</p>

<p>As illustrated in the diagram below, a 3D mesh exists in its own coordinate system within the larger 3D space. The rendering pipeline transforms the vertices of this mesh into the 2D pixel grid of the camera's view.</p>

<div class="image-row">
    <div class="image-container">
        <img src="3d_scene.png" width="700"/>
        <div class="image-caption">The rendering pipeline transforms 3D meshes into 2D images through projection and shading</div>
    </div>
</div>

<ul>
    <li><strong>Projection:</strong> This is the geometric heart of rendering. The engine takes the list of 3D vertices from our mesh and multiplies them by the world-to-camera matrix and then by the perspective camera-to-view matrix. This series of matrix multiplications is a powerful mathematical operation that effectively projects each 3D vertex onto the camera's 2D image plane. The result is a new set of 2D vertices that represent the object's silhouette from the camera's point of view.</li>
    
    <li><strong>Shading (or Rasterization):</strong> Simply projecting the vertices gives us a 2D wireframe. To create a solid, realistic image, we need to fill in the pixels. This step, often called rasterization or shading, determines the final color of each pixel covered by our 2D projected shape. The graphics pipeline iterates over the pixels within the projected triangles of the mesh and calculates their color based on various factors: the object's base color (texture), its material properties (e.g., how shiny or rough it is), the location and intensity of lights in the scene, and whether the pixel is in shadow. The result is the final shaded, textured, and lit 2D image that we see on our screen.</li>
</ul>

<h2 id="reconstructing-3d-scene">Reconstructing 3D Scene</h2>

<h3>Inputs and Initial Problems</h3>

<p>The journey of reconstruction typically begins with a set of overlapping images of a scene, often sourced from the frames of a video 📹. This collection of 2D views is our only input. From this, we must solve two fundamental problems before we can even think about building a 3D model.</p>

<div class="image-row">
    <div class="image-container">
        <img src="triangulation.png" width="600"/>
        <div class="image-caption">Triangulation: By matching the same point across multiple camera views,<br> we can determine its 3D position in space.<br>(Image source: <a href="https://www.researchgate.net/figure/Schematic-of-the-triangulation-in-3D-reconstruction-Identical-points-across-the-images_fig83_342869212" target="_blank">3D Reconstruction from Multiple Images</a>)</div>
    </div>
</div>

<p>First is <strong>camera pose retrieval</strong>. As a camera moves through a scene, its position and orientation change with every frame. We must accurately calculate this 6-DoF (six degrees of freedom: 3 for position, 3 for rotation) pose for each image to understand where it was taken from.</p>

<p>Second, we must determine the <strong>depth for each pixel</strong>. Depth estimation tells us how far every point in the scene is from the camera, transforming our flat images into a spatial map.</p>

<p>Solving for camera poses and depth across multiple views is a complex geometric puzzle known as <strong>Structure from Motion (SfM)</strong> and <strong>Multi-View Stereo (MVS)</strong>.</p>

<h3>From Problems to Representations</h3>

<p>Once we have a handle on the camera poses and depth maps, we can begin to build the actual 3D representation. The choice of representation dictates the nature and quality of the final model:</p>

<ul>
    <li><strong>Point Clouds:</strong> By taking the depth value of a pixel and projecting it into 3D space using its corresponding camera pose, we can create a point cloud. This is the most direct output of the initial reconstruction steps - a raw, foundational skeleton of the scene. While useful for understanding the basic geometry, this representation is shallow because it consists only of disconnected points and lacks surfaces for realistic rendering.</li>
    
    <div class="image-row">
        <div class="image-container">
            <img src="3d_point_cloud.png" width="500"/>
            <div class="image-caption">A 3D point cloud reconstructed from multiple images of a turtle</div>
        </div>
    </div>

    <li><strong>Explicit Meshes:</strong> To create a more solid and tangible model, we can process a point cloud to generate an explicit mesh. Algorithms connect the individual 3D points to form a continuous surface of polygons (usually triangles). This gives us a watertight model that can be properly textured and lit, making it ideal for applications like games or simulations.</li>
    
    <div class="image-row">
        <div class="image-container">
            <img src="3d_mesh_refiningpng.png" width="300"/>
            <div class="image-caption">3D mesh-triangles with different resolution<br>(Image source: <a href="https://cathyatseneca.gitbooks.io/3d-modelling-for-programmers/content/3ds_max_basics/3d_representation.html" target="_blank">3D Modelling for Programmers</a>)</div>
        </div>
    </div>

    <li><strong>Implicit Representations (NeRFs):</strong> A more modern approach bypasses creating an explicit mesh altogether. A Neural Radiance Field (NeRF) is a neural network that acts as an implicit representation. It learns a continuous function directly from the images and their camera poses. By feeding the network a 3D coordinate and a viewing direction, it outputs the color and density at that point. This allows for the rendering of incredibly photorealistic novel views by effectively learning both the geometry (like depth) and appearance (color and light interaction) of the entire scene at once.</li>
    <div class="image-row">
        <div class="image-container">
            <img src="nerf.png" width="700"/>
            <div class="image-caption">NeRF pipeline: rays are sampled through a scene and processed by a neural network.<br>(Image source: <a href="https://www.blakegella.com/posts/Computer-Vision-001-Neural-Radiance-Fields/" target="_blank">blakegella.com</a>)</div>
        </div>
    </div>
    
    <li><strong>Hybrid Representations (3D Gaussian Splatting):</strong> Occupying a powerful middle ground, 3D Gaussian Splatting has recently emerged. This technique takes the initial point cloud (generated from SfM) and converts each point into a 3D Gaussian - a soft, colored, transparent blob. These Gaussians are an explicit representation that can be "splatted" or projected onto a 2D plane with extreme efficiency. This hybrid approach achieves the photorealism of NeRFs but with the significant advantage of real-time rendering speeds, marking a major leap forward for the field.</li>
    <div class="image-row">
        <div class="image-container">
            <img src="3dgs.png" width="350"/>
            <div class="image-caption">3D Gaussian Splatting: 3D Gaussians are projected into image for rendering.<br>(Image source: <a href="https://arxiv.org/pdf/2401.03890" target="_blank">A Survey on 3D Gaussian Splatting</a>)</div>
        </div>
    </div>

</ul>

<h2 id="3d-point-clouds-with-colmap">3D Point Clouds with COLMAP</h2>

<p>Now that we understand the reconstruction challenge, let's see how COLMAP actually solves it. Think of COLMAP as a geometric detective - it examines a collection of photos and figures out both where each camera was positioned and the 3D structure of the scene.</p>

<h3>The Three-Stage Process</h3>

<p><strong>Feature Detection:</strong> COLMAP scans each image for distinctive visual landmarks - corners, edges, and unique patterns that can be recognized from different angles. Using SIFT features, it creates mathematical "fingerprints" for thousands of points per image.</p>

<p><strong>Feature Matching:</strong> Like a matchmaker, COLMAP compares these fingerprints across all image pairs. Here's where the computational reality hits - with $N$ images, COLMAP must perform $N \times (N-1)/2$ pairwise comparisons. For a modest 100-image dataset, that's 4,950 image pairs to analyze. For matching process COLMAP uses SIFT(Scale-Invariant Feature Transform) features. Each comparison involves matching potentially thousands of features, making this stage scale quadratically with the number of input images.</p>

<div class="image-row">
    <div class="image-container">
        <img src="sift_matching.png" width="450"/>
        <div class="image-caption">SIFT feature matching between two views of the same scene<br>(Image source: <a href="https://www.slideserve.com/sharonmoore/the-sift-scale-invariant-feature-transform-detector-and-descriptor-powerpoint-ppt-presentation" target="_blank">SIFT</a>)</div>
    </div>
</div>

<p><strong>Bundle Adjustment:</strong> The magic happens here, but it's computationally expensive magic. COLMAP simultaneously estimates camera poses and triangulates 3D point positions through global optimization. This involves solving large sparse systems with thousands of variables, often taking hours for complex scenes.</p>

<h3>The Computational Cost</h3>

<p>The quadratic complexity means processing time explodes with dataset size. A 50-image sequence might take 30 minutes, while 200 images could require 8+ hours on the same hardware. The feature matching stage alone can consume 70-80% of the total runtime, as COLMAP exhaustively searches for correspondences across all image pairs.</p>

<h3>The Result: A Sparse Foundation</h3>

<p>What emerges after this lengthy process is a sparse but highly accurate 3D point cloud - a constellation of reliable points that multiple cameras agree upon. Each point represents a location verified through geometric constraints across multiple views.</p>

<div class="image-row">
    <div class="image-container">
        <img src="colmap_input.png" width="350" height="250" style="object-fit: cover;"/>
        <div class="image-caption">Input: Multi-view images</div>
    </div>
    <div class="image-container">
        <img src="colmap_output.gif" width="350" height="250" style="object-fit: cover; cursor: pointer;" onclick="this.src=this.src;" title="Click to replay animation"/>
        <div class="image-caption">Output: Sparse point cloud with camera poses</div>
    </div>
</div>
<div style="text-align: center; font-style: italic; color: #666; font-size: 16px; margin-top: 10px;">
    COLMAP workflow: from input images to sparse 3D reconstruction<br>
    (Image source: <a href="https://peterfalkingham.com/2017/04/04/photogrammetry-testing-8-colmap/" target="_blank">Peter Falkingham</a>)
</div>



<p>This sparse output serves as the geometric backbone for all advanced techniques. Every modern 3D reconstruction method - from NeRFs to Gaussian Splatting - builds upon the camera poses and sparse geometry that COLMAP provides.</p>

<p>COLMAP's reliability comes at a computational cost, driving researchers to seek faster alternatives - which is exactly what we'll explore next with DUST3R.</p>

<h2 id="modern-alternatives-dust3r-and-mast3r">Modern Alternatives: DUST3R and MASt3R-SFM</h2>

<p>What if we could skip the entire classical SfM pipeline altogether? Instead of detecting keypoints, matching them, and triangulating - what if a neural network could directly predict 3D structure from raw pixels?</p>

<h3>DUST3R: Direct 3D Prediction</h3>

<p>DUST3R takes a radically different approach. Given two images, it directly regresses dense "pointmaps" - essentially a 3D coordinate for every pixel. The core insight is elegant: rather than building complex pipelines to infer 3D structure, train a transformer to predict it directly.</p>

<p>The network architecture follows a Siamese design where both images are encoded through shared ViT encoders, then processed by twin decoders that output pointmaps expressed in the first camera's coordinate frame. This shared coordinate system is crucial - it means the 3D points from both views are already aligned, eliminating the need for complex pose estimation procedures.</p>

<div class="image-row">
    <div class="image-container">
        <img src="dust3r_pipeline.png" width="700"/>
        <div class="image-caption">DUST3R pipeline: Direct pointmap prediction from image pairs<br>(Image source: <a href="https://arxiv.org/abs/2312.14132" target="_blank">DUSt3R</a>)</div>
    </div>
</div>

<p>The magic happens in training: DUST3R learns geometric priors from massive datasets, developing an intuitive understanding of 3D structure that often surpasses handcrafted algorithms. It handles challenging cases where classical methods fail: minimal overlap between views, textureless regions, or insufficient camera motion.</p>

<p>But here's the catch - DUST3R processes image pairs. For N images, you need N² forward passes. With a typical transformer requiring substantial memory, this becomes prohibitive beyond ~50 images. The method trades classical complexity for neural complexity, but the quadratic scaling remains.</p>

<h3>MASt3R-SfM: Making Neural SfM Scale</h3>

<p>MASt3R-SfM tackles DUST3R's scalability head-on with two key innovations.</p>

<p><strong>Richer representations:</strong> Instead of just predicting 3D coordinates, MASt3R outputs dense feature descriptors for each pixel patch. These learned features capture local geometric and appearance information, enabling robust matching between pointmaps even when 3D coordinates alone might be ambiguous.</p>

<p><strong>Graph-based processing:</strong> Rather than exhaustively processing all pairs, MASt3R-SfM builds a sparse scene graph. The frozen MASt3R encoder - originally designed for 3D prediction - turns out to be an excellent image retriever. Using ASMK aggregation on encoder features, it identifies likely overlapping pairs with minimal computational overhead.</p>

<div class="image-row">
    <div class="image-container">
        <img src="mast3r_sfm_pipeline.png" width="700"/>
        <div class="image-caption">MASt3R-SFM pipeline: encoding, graph-based processing, decoding and two-stage optimization<br>(Image source: <a href="https://arxiv.org/pdf/2409.19152"
             target="_blank">MAStR-SFM</a>)</div>
    </div>
</div>

<p>The resulting graph contains only O(N) edges instead of O(N²). A typical approach connects images to k=10 nearest neighbors plus a sparse set of "keyframe" images, creating a connected graph with linear complexity.</p>

<p>Two-stage optimization then aligns these local reconstructions globally. First, pointmaps are rigidly aligned in 3D space using the learned correspondences. Then a refinement stage minimizes 2D reprojection errors, similar to bundle adjustment but operating on the neural predictions rather than classical triangulated points.</p>

<h3>Why This Matters</h3>

<p>The complexity difference is dramatic:</p>

<ul>
    <li><strong>COLMAP:</strong> O(N²) feature matching with expensive RANSAC iterations</li>
    <li><strong>DUST3R:</strong> O(N²) transformer forward passes, memory-bound</li>
    <li><strong>MASt3R-SfM:</strong> O(N) forward passes + O(N²) retrieval (but retrieval is fast)</li>
</ul>

<p>In practice, MASt3R-SfM handles 200+ images where DUST3R runs out of memory at 50. The neural approach also shows remarkable consistency - while classical methods degrade significantly with fewer views or challenging motion, MASt3R-SfM maintains steady performance from 3 to 100+ images.</p>

<p>MASt3R-SfM works on truly unconstrained image collections. No assumptions about camera motion patterns, no requirements for high overlap, no manual parameter tuning. You feed it photos, it gives you cameras and 3D structure.</p>

<div class="image-row">
    <div class="image-container">
        <img src="mast3r_panther_out.png" width="350" height="260" style="object-fit: cover;"/>
    </div>
    <div class="image-container">
        <img src="mast3r_panther_out.gif" width="350" height="260" style="object-fit: cover;"/>
    </div>
</div>
<div style="text-align: center; font-style: italic; color: #666; font-size: 16px; margin-top: 10px;">
    Point cloud and camera outputs generated by the MASt3R method
</div>

<h2 id="nerfs-neural-radiance-fields">NeRFs: Neural Radiance Fields</h2>

<p>We've extracted sparse point clouds and camera poses from our image collections. Whether using COLMAP's methodical geometric analysis or MASt3R-SfM's neural predictions, we end up with the same foundation: a constellation of reliable 3D points and precisely calibrated camera positions.</p>

<p>Those sparse points are just the skeleton. Between each reconstructed point lies vast empty space, and our original training images contain rich photographic detail that no point cloud can capture. Neural Radiance Fields (NeRFs) fill this gap.</p>

<h3>The NeRF Architecture</h3>

<p>A NeRF represents the entire scene as a continuous function. Feed the network any 3D coordinate $(x,y,z)$ and viewing direction $(\theta,\phi)$, and it outputs the color $(r,g,b)$ and density $(\sigma)$ at that point:</p>

<p>$$F(x,y,z,\theta,\phi) \rightarrow (r,g,b,\sigma)$$</p>

<p>The scene becomes a learned, continuous field of radiance values. This is fundamentally different from discrete representations—every point in 3D space has a potential color and opacity.</p>

<h3>Volume Rendering Process</h3>

<p>Training follows a computationally intensive pattern. For each training image, rays are cast from the camera center through every pixel into the 3D scene. Along each ray, multiple 3D points are sampled and queried through the neural network.</p>
<div class="image-row" style="margin-top: 20px; margin-bottom: 10px;">
    <div class="image-container" style="text-align: center;">
        <img src="sampling_nerf.png" alt="NeRF volume sampling illustration" width="700" style="max-width: 100%; object-fit: contain;"/>
        <div style="font-size: 15px; color: #666; margin-top: 6px;">
            Example of how the volume is sampled along rays and the neural network is trained.<br>
            Image source: <a href="https://ar5iv.labs.arxiv.org/html/2111.11426" target="_blank">Neural Fields</a>
        </div>
    </div>
</div>

<p>The key insight is volume rendering. Instead of finding surface intersections, NeRF accumulates color and opacity along the entire ray. Points with high density contribute more to the final pixel color, while transparent regions allow deeper points to show through. This differentiable process enables backpropagation of pixel-level losses directly to the network weights.</p>

<h3>The Overfitting Strategy</h3>

<p>NeRFs deliberately overfit to their training data. The network must memorize how light behaves in this specific environment, learning not just the geometry we reconstructed, but also complex material interactions, shadows, and volumetric effects.</p>

<p>This process requires thousands of training iterations, typically taking <strong>6–12 hours</strong> on modern GPUs for a single scene. The network encodes every lighting condition, surface detail, and shadow captured in the training images.</p>

<h3>The Payoff: Photorealistic Novel Views</h3>

<p>Once trained, the NeRF can render photorealistic images from any camera position within the training distribution. The results often surpass traditional mesh-based rendering, naturally handling view-dependent effects like reflections, volumetric phenomena like smoke, and fine geometric details. The continuous nature enables rendering at any resolution with smooth, artifact-free camera motion.</p>

<p>Modern implementations like Instant-NGP have reduced training time from hours to minutes, but the fundamental trade-off remains. Creating a neural representation that can synthesize photorealistic novel views requires substantial computation to overfit the network to your specific scene.</p>

<h2 id="3d-gaussian-splatting">3D Gaussian Splatting</h2>

<p>We've seen how NeRFs achieve photorealistic novel views by learning continuous radiance fields. However, NeRFs have a critical limitation: rendering speed. Even optimized versions require seconds per image, making them unsuitable for interactive applications like gaming or AR.</p>

<p>3D Gaussian Splatting (3DGS) bridges this gap by representing scenes as collections of 3D Gaussian primitives instead of neural networks. This approach maintains NeRF-quality visuals while achieving real-time rendering speeds.</p>

<h3>The Gaussian Representation</h3>

<p>Instead of querying a neural network along camera rays, 3DGS represents the scene using millions of 3D Gaussians—colored, semi-transparent ellipses positioned throughout space. Think of it as replacing NeRF's smooth function with a pointillist painting made of 3D brushstrokes.</p>

<p>Each 3D Gaussian Splat stores:</p>

<ul>
    <li><strong>Center Position $\mu \in \mathbb{R}^3$:</strong> Specifies the central location of the Gaussian in the 3D space.</li>
    <li><strong>Color Representation:</strong> Utilizes spherical harmonics (SH) coefficients $c \in \mathbb{R}^k$ to encode the color information, where $k$ represents the degrees of freedom in the color model.</li>
    <li><strong>Rotation Factor $r \in \mathbb{R}^4$:</strong> Defined in quaternion terms to manage the orientation of the Gaussian.</li>
    <li><strong>Scale Factor $s \in \mathbb{R}^3$:</strong> Determines the size of the Gaussian along each axis, forming an ellipsoidal shape.</li>
    <li><strong>Opacity $\alpha \in \mathbb{R}$:</strong> Controls the transparency.</li>
</ul>

<div class="image-row" style="margin-top: 20px; margin-bottom: 10px;">
    <div class="image-container" style="text-align: center;">
        <img src="3dgs_attributes.png" alt="3DGS Densification" width="700" style="max-width: 100%; object-fit: contain;"/>
        <div style="font-size: 15px; color: #666; margin-top: 6px;">
            How 3DGS parameters optimization influence 3D Gaussian Splat.
        </div>
    </div>
</div>

<p>When rendering, these 3D Gaussians project onto the image plane and blend together using alpha compositing—a process that's highly parallelizable on modern GPUs.</p>

<h3>Training: Sculpting with Mathematics</h3>

<p>Like NeRFs, 3DGS requires camera poses and benefits enormously from sparse point clouds (from COLMAP or MASt3R-SfM). Each sparse point becomes an initial Gaussian that optimization then refines.</p>

<p>The training objective minimizes reconstruction error:</p>

<p>$$L = (1 - \lambda)L_1 + \lambda L_{D-SSIM}$$</p>

<p>The real innovation is <strong>adaptive density control</strong>. During optimization, the system continuously monitors reconstruction quality and automatically adjusts the representation:</p>

<ul>
    <li><strong>Densification:</strong> In under-reconstructed regions, large Gaussians split into smaller ones (like cell division), while new Gaussians clone in sparse areas.</li>
    <div class="image-row" style="margin-top: 20px; margin-bottom: 10px;">
        <div class="image-container" style="text-align: center;">
            <img src="3dgs_density_control.png" alt="3DGS Densification" width="400" style="max-width: 100%; object-fit: contain;"/>
            <div style="font-size: 15px; color: #666; margin-top: 6px;">
                Two densification strategies for 3DGS for over and under - reconstructed regions.<br>
                Image source: <a href="https://arxiv.org/pdf/2308.04079" target="_blank">3DGS Densification</a>
            </div>
        </div>
    </div>
    <li><strong>Pruning:</strong> Gaussians with low opacity or minimal contribution get removed.</li>
</ul>



<p>This creates organic sculpting where complexity adapts to scene requirements. Detailed areas like foliage develop dense clusters of small Gaussians, while smooth surfaces use fewer, larger ones.</p>

<h3>Performance Revolution</h3>

<p>The performance difference is dramatic. 3DGS trains in <strong>15-30 minutes</strong> compared to NeRF's <strong>6-12 hours</strong> a 10-20x speedup. This comes from the explicit representation requiring no expensive neural network evaluations during optimization.</p>

<p>Rendering performance is even more impressive: <strong>30-60+ FPS</strong> at high resolutions. The rasterization process (project, sort by depth, alpha blend) maps perfectly to graphics hardware, unlike NeRF's complex ray marching.</p>

<h3>Beyond Rendering: Analysis and Editing</h3>

<p>The explicit nature unlocks capabilities impossible with NeRFs. Since scenes consist of discrete Gaussians with interpretable parameters, you can:</p>

<ul>
    <li>Analyze structure by examining Gaussian distributions</li>
    <li>Edit scenes by moving, removing, or modifying individual Gaussians</li>
    <li>Extract semantics by clustering Gaussians with similar properties</li>
    <li>Compress by pruning redundant elements</li>
</ul>

<p>Researchers have demonstrated removing objects, changing materials, and animating static scenes—all by manipulating the underlying Gaussian representation.</p>

<h3>Technical Considerations</h3>

<p>3DGS initialization strongly depends on sparse point cloud quality. Poor initial geometry leads to longer training and potential artifacts, especially for camera positions far from training views.</p>

<p>Memory scales with scene complexity, typical outdoor scenes require 1-5 million Gaussians. However, this explicit storage often proves more efficient than the large neural networks needed for comparable NeRF quality.</p>

<p>The method can struggle with complex optical effects. Highly reflective surfaces or intricate volumetric phenomena may not be perfectly captured by discrete Gaussians, though visual quality typically remains high for practical scenes.</p>

<h2 id="from-theory-to-practice">From Theory to Practice</h2>

<h2 id="references">References</h2>

<ul>
    <li>Wikipedia contributors. "Polygon mesh." Wikipedia, The Free Encyclopedia. <a href="https://en.wikipedia.org/wiki/Polygon_mesh" target="_blank">https://en.wikipedia.org/wiki/Polygon_mesh</a> (accessed 2025).</li>
</ul>

</div>
</article>

</main>

<footer class="footer">
    <div class="footer-content">
        <div class="footer-links">
            <a href="https://ostapagon.github.io/">Home</a>
            <a href="https://ostapagon.github.io/posts/">Posts</a>
            <a href="https://github.com/ostapagon">GitHub</a>
            <a href="https://twitter.com/ostapagone">Twitter</a>
            <a href="https://www.linkedin.com/in/ostap-hembara/">LinkedIn</a>
        </div>
    </div>
    <div class="footer-bottom">
        <p>&copy; 2024 ostapagon. All rights reserved.</p>
    </div>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<!-- Fallback for highlight.js -->
<script>
function loadHighlightJSFallback() {
    console.log('Primary highlight.js failed, loading fallback from CDN');
    var script = document.createElement('script');
    script.src = 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js';
    script.onload = function() {
        hljs.highlightAll();
        console.log('Fallback highlight.js loaded and initialized');
    };
    script.onerror = function() {
        console.error('Both primary and fallback highlight.js failed to load');
    };
    document.head.appendChild(script);
    
    // Also load Python language support
    var pythonScript = document.createElement('script');
    pythonScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js';
    pythonScript.onload = function() {
        hljs.highlightAll();
    };
    document.head.appendChild(pythonScript);
}

// Additional debugging and manual initialization
document.addEventListener('DOMContentLoaded', function() {
    // Check if highlight.js loaded
    if (typeof hljs === 'undefined') {
        console.log('highlight.js not loaded, attempting fallback');
        loadHighlightJSFallback();
    } else {
        console.log('highlight.js loaded successfully');
        // Ensure all code blocks are highlighted
        setTimeout(function() {
            hljs.highlightAll();
        }, 100);
    }
});
</script>



</body>
</html> 